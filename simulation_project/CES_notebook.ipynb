{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Rethinking the Simulated Environment\n",
    "---\n",
    "\n",
    "There are three components to the simulated environment.\n",
    "1. production function\n",
    "2. reward function\n",
    "3. hydrology function\n",
    "\n",
    "Marco and I mainly discussed the simulated environment and how to best model it.\n",
    "After our meeting, I think it will be beneficial to significantly change the first and third components.\n",
    "Changing two components leads to rethinking the reward function. Bellow we will examine the previous version of each\n",
    "component and the proposed changes.\n",
    "\n",
    "Lastly, Marco argued that the simulation should operate at the timescale of one year to integrate appropriately\n",
    "with the new production function and to make the simulation more tractable within the scope of a master's thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Production function\n",
    "In the past iteration the production function was a biologically based function with water/day as input and growth/day\n",
    "as output. Marco recommended changing the production function to an economically based production function.\n",
    "This serves two purposes. It simplifies calibrating the production function to real world data and, as we will see in the\n",
    "next section, it will integrate better with the reward function.\n",
    "\n",
    "### Old production function:\n",
    "$$ Q = K *log(x_{water})$$\n",
    "\n",
    "$K = $ production constant for scaling\n",
    "\n",
    "$x_{water} = $ amount of water applied to the crop\n",
    "\n",
    "### New Production Function: Constant Elasticity of Substitution (CES)\n",
    "\n",
    "CES is an aggregation production function with two or more production inputs that have some\n",
    "elasticity of substitution.\n",
    "\n",
    "$$ Q_i = \\pi_i (\\beta_{land} * x^\\rho_{land} + \\beta_{water} * x^\\rho_{water})^{r/\\rho} $$\n",
    "\n",
    "$$ \\beta_{land} + \\beta_{water} = 1 $$\n",
    "\n",
    "$ Q_i =$ quantity of output, pounds of crop $i$\n",
    "\n",
    "$\\pi_i = $ factor productivity, normalizes units, from real world data\n",
    "\n",
    "$\\beta = $ share parameter, measures how important each input is, from real world data\n",
    "\n",
    "$\\rho = $ substitution parameter, how willing is the agent to swap $x_{land}$ for $x_{water}$, from real world data\n",
    "\n",
    "$r = $ returns to scale, does crop $ i $ have economies of scale, from real world data\n",
    "\n",
    "$ x_{input} = $ quantity of given input, agents action space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What will change\n",
    "\n",
    "old state vector:\n",
    "\n",
    "- available_water\n",
    "- CropWater_n\n",
    "- CropGrowth_n\n",
    "- CropPrices_n\n",
    "\n",
    "new state vector:\n",
    "\n",
    "- available_water\n",
    "- available_land\n",
    "- one_hot_available_crops_vector\n",
    "- crop price vector\n",
    "\n",
    "Possible additions:\n",
    "- Hidden state vector\n",
    "\n",
    "old action vector:\n",
    "- water for each crop\n",
    "\n",
    "new action vector:\n",
    "- water for each crop\n",
    "- land for each crop\n",
    "\n",
    "I would love to hear your feedback on the state vector. Does it make sense to have the previous land crop and reward?\n",
    "This is reminiscent of an LSTM but is also somewhat similar to the occasional myopic view of farmers.\n",
    "\n",
    "The production function Could also add additional inputs such as fertilizer ect."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Reward Function\n",
    "\n",
    "Since this new production function returns weights for each crop, with a given price/weight we can calculate the reward\n",
    "in dollars. Not so interesting. The more interesting result is that if dollars are our return scalar, then all other rewards\n",
    "must be stated in dollars. For example, if we want to incorporate a return for leaving water in the river for the farmer\n",
    "downstream farmer or for the ecological health of the river, we need to state that reward in dollars. To do this, Marco\n",
    "recommended I use another economic concept called a shadow price. A shadow price aims to put a dollar value on the\n",
    "externalities for a given action or good. I have not looked into what real world values for this look like, but it wil be the first step after\n",
    "getting a minimum viable simulation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Hydrology function\n",
    "\n",
    "The main change to the hydrology function comes from the change in the time step for each episode. Since we switch to\n",
    "a yearly interval for the production function, we need to change the hydrology model to switch to a yearly function that\n",
    "still incorporate stochastic water supply.\n",
    "\n",
    "Old hydrology function:\n",
    "\n",
    "$$ W_t = A * sin(\\omega t + \\phi) $$\n",
    "\n",
    "A standard oscillation formula.\n",
    "\n",
    "New hydrology function:\n",
    "\n",
    "$$ W_t \\in P(W \\vert L_{ocation}) $$\n",
    "\n",
    "Now we sample from a distribution given some geographic location where the distribution is derived from real world data.\n",
    "\n",
    "\n",
    "This will allow us to do nice things once the agents are trained such as skew or change the distribution\n",
    "to simulate changing water availability due to climate change or some other factor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class SimConfig:\n",
    "    \"\"\"\n",
    "    This config class simplifies the initialization of the SimulationCES class.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        self.num_crops = 2\n",
    "\n",
    "        # TODO: use a dictionary to hold these values for different watersheds, then access watershed values via init argument\n",
    "        # hydrology function, current estimate based on lower Clark Fork\n",
    "        self.water_mu = 1.2e6       # cfs\n",
    "        self.water_sigma = 1e3      # cfs\n",
    "        self.aquifer_volume = self.water_mu * 2 # cf\n",
    "\n",
    "        # stochastic water dist\n",
    "        self.water_dist = self.water_sigma * np.random.randn(100) + self.water_mu\n",
    "        # # constant water dist\n",
    "        # self.water_dist = np.full(100, self.water_mu)\n",
    "\n",
    "        # simulation parameters\n",
    "        self.number_farmers = 3\n",
    "        self.farmer_priority = [0, 1, 2]\n",
    "        self.random_seed = 0\n",
    "\n",
    "        # agent parameters\n",
    "        # [available_water, available_land, crop_identity_vec, crop_price_vec]\n",
    "        # FIXME: THIS WILL NEED TO CHANGE WHEN self.crop_list CHANGES TO ONE HOT VECTOR\n",
    "        self.state_size = 3\n",
    "        self.action_size = 2 * self.num_crops\n",
    "        self.memory_size = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Aquifer:\n",
    "    # TODO: CURRENTLY THIS IS INCORPORATED IN THE SAME PRIORITY AS SURFACE WATER, CHANGING THIS MAY LEAD TO MORE INTERESTING DYNAMICS\n",
    "\n",
    "    def __init__(self, initial_volume):\n",
    "        self.available_volume = initial_volume\n",
    "        self.recharge_mu = initial_volume * 0.1\n",
    "        self.recharge_sigma = self.recharge_mu * 0.3\n",
    "\n",
    "    def withdraw_water(self, amount):\n",
    "        \"\"\"\n",
    "        Withdraws water from the aquifer.\n",
    "        Currently allows for complete draining of the aquifer but prevents negative values.\n",
    "\n",
    "        Amount in the aquifer is unknown to the farmer and therefore is not in the agent's state vector.\n",
    "        :param amount: attempted water withdrawal\n",
    "        :return: actual amount withdrawn\n",
    "        \"\"\"\n",
    "        if amount < self.available_volume:\n",
    "            self.available_volume -= amount\n",
    "\n",
    "            return amount\n",
    "\n",
    "        else:\n",
    "            amount = self.available_volume\n",
    "            self.available_volume = 0\n",
    "\n",
    "            return amount\n",
    "\n",
    "    def recharge_aquifer(self):\n",
    "        \"\"\"\n",
    "        Stochastic recharge of the aquifer.\n",
    "        Currently based in independent distribution but could be tied to surface water.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.available_volume += np.random.randn() * self.recharge_sigma + self.recharge_mu\n",
    "\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-9ffeeca8459b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi_episode\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_years\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 273\u001B[0;31m     \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    274\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-8-9ffeeca8459b>\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    243\u001B[0m             \u001B[0;31m# get action, and record actions for debugging\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 244\u001B[0;31m             \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    245\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfarmers_actions_record\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpriority_num\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    246\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/MS_Thesis/simulation_project/CES_Ag_agent.py\u001B[0m in \u001B[0;36mact\u001B[0;34m(self, state, add_noise)\u001B[0m\n\u001B[1;32m     73\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mactor_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 75\u001B[0;31m             \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mactor_local\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     76\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mactor_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    720\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    721\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 722\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    724\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/MS_Thesis/simulation_project/CES_A2C_Model.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[0mwater_actions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax_water\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mtotal_water_proportion\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mcrop_water_proportions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 84\u001B[0;31m         \u001B[0mland_actions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax_land\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mtotal_land_proportion\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mcrop_land_proportions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     85\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwater_actions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mland_actions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/MS_Thesis/simulation_project/CES_A2C_Model.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[0mwater_actions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax_water\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mtotal_water_proportion\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mcrop_water_proportions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 84\u001B[0;31m         \u001B[0mland_actions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax_land\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mtotal_land_proportion\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mcrop_land_proportions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     85\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwater_actions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mland_actions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/211.6693.115/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mtrace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    746\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpydev_state\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mSTATE_SUSPEND\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 747\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    748\u001B[0m                     \u001B[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    749\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrace_dispatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/211.6693.115/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[0;31m# IFDEF CYTHON\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/211.6693.115/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1147\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/211.6693.115/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1162\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from simulation_project.CES_Ag_agent import Agent\n",
    "\n",
    "class SimulationCES:\n",
    "    \"\"\"\n",
    "    Simulation environment utilizing CES production funtion\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: SimConfig):\n",
    "        # simulation configuration\n",
    "        self.config = config\n",
    "\n",
    "        # simulation records for analysis\n",
    "        self.farmers_rewards_record = [[] for _ in range(config.number_farmers)]\n",
    "        self.water_record = {\"Aquifer\": [],\n",
    "                             \"Surface\": []}\n",
    "        self.farmers_water_withdrawal_record = [[] for _ in range(config.number_farmers)]\n",
    "        self.farmers_actions_record = [[] for _ in range(config.number_farmers)]\n",
    "\n",
    "        # simulation parameters\n",
    "        self.num_farmers = config.number_farmers\n",
    "        self.farmer_priority = config.farmer_priority\n",
    "        self.farmer_list = [Agent(config.state_size, config.action_size, config.random_seed) for _ in range(self.num_farmers)]\n",
    "        self.year = 1\n",
    "\n",
    "        # simulation functions, reward function is a property\n",
    "        self.hydrology_function = config.water_dist\n",
    "        self.aquifer = Aquifer(config.aquifer_volume)\n",
    "\n",
    "        # initialize river continuum\n",
    "        self.source_water = self.init_available_water()\n",
    "        self.available_water = [self.source_water for _ in range(self.num_farmers)]\n",
    "\n",
    "        # land available to each farmer\n",
    "        # FIXME: MAKE THIS COME FROM CONFIG FILE?\n",
    "        self.available_land = [100 for _ in range(self.num_farmers)]\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: use a dictionary to hold these values for different watersheds, then access watershed values via init argument\n",
    "        # production function parameters\n",
    "        self.crop_1 = {\"pi\": 10,\n",
    "                       \"beta_land\": 0.5,\n",
    "                       \"beta_water\": 0.5,\n",
    "                       \"rho\": 1,\n",
    "                       \"r\": 1}\n",
    "        # self.crop_2 = {\"pi\": 10,\n",
    "        #                \"beta_land\": 0.3,\n",
    "        #                \"beta_water\": 0.7,\n",
    "        #                \"rho\": 1,\n",
    "        #                \"r\": 1}\n",
    "\n",
    "        # TODO: use historic data to determine price distribution and sample from it each year to dynamically allocate price\n",
    "        # TODO: DETERMINE IF ABOVE IS A REASONABLE ASSUMPTION GIVEN COMMODITY PRICE GUARANTEES.\n",
    "        # TODO: ARRANGE THIS INFO INTO A VECTOR INHERENTLY, THIS WILL REQUIRE CHANGING THE REWARD FUNCTION CALC\n",
    "        # reward function, has been turned into an @property\n",
    "        # self.reward_function = self.reward_function()\n",
    "\n",
    "        # TODO: THESE NEED TO GO INTO THE CONFIG CLASS\n",
    "        self.crop_1_price = 10\n",
    "        self.crop_2_price = 15\n",
    "        self.crop_prices = [self.crop_1_price]\n",
    "\n",
    "        # cost parameters\n",
    "        # TODO: IMPROVE FLEXIBILITY\n",
    "        # NOT IN USE\n",
    "        # self.crop_list = np.array([1, 1])\n",
    "        self.cost_mu = np.array([2, 3])\n",
    "        self.cost_sigma = np.array([0.5, 0.5])\n",
    "        # FIXME: FARMER HIDDEN STATE VECTOR, maybe\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the simulation environment. Open AI Gym interface.\n",
    "\n",
    "        Only the river continuum needs to be reset each episode.\n",
    "        Everything else needs to persist between episodes. I think...\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "\n",
    "        # recharge aquifer\n",
    "        self.aquifer.recharge_aquifer()\n",
    "\n",
    "        # re-initialize available water, this includes surface and aquifer\n",
    "        self.source_water = self.init_available_water()\n",
    "        self.available_water = [self.source_water for _ in range(self.num_farmers)]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def reward_function(self, action):\n",
    "\n",
    "        # FIXME: DEAL WITH THIS UNPACKING MORE ELEGANTLY\n",
    "        x1_water, x2_water, x2_land,  x1_land  = action\n",
    "        # FIXME: MAY INPUT A np.array FOR FLEXIBILITY\n",
    "        \"\"\"input amounts are the action space for the agents\"\"\"\n",
    "        q_1 = self.crop_1[\"pi\"] * (self.crop_1[\"beta_land\"] * (x1_land ** self.crop_1[\"rho\"]) + self.crop_1[\"beta_water\"] * (x1_water ** self.crop_1[\"rho\"])) ** (self.crop_1[\"r\"] / self.crop_1[\"rho\"])\n",
    "        # q_2 = self.crop_2[\"pi\"] * (self.crop_2[\"beta_land\"] * (x2_land ** self.crop_2[\"rho\"]) + self.crop_2[\"beta_water\"] * (x2_water ** self.crop_2[\"rho\"])) ** (self.crop_2[\"r\"] / self.crop_2[\"rho\"])\n",
    "\n",
    "        r_1 = np.log(q_1 * self.crop_1_price)\n",
    "        # r_2 = q_2 * self.crop_2_price\n",
    "\n",
    "        land_cost = self.land_cost_function(x1_land + x2_land)\n",
    "        water_cost = self.water_cost_function(x1_water + x2_water)\n",
    "        total_cost = land_cost + water_cost\n",
    "\n",
    "        # TODO: EXPAND THIS, it needs to be temporally correlated\n",
    "        shadow_price = 0\n",
    "\n",
    "        return r_1  - total_cost - shadow_price\n",
    "\n",
    "    def water_cost_function(self, total_water):\n",
    "        \"\"\"\n",
    "        Non-Monotonic cost function for water.\n",
    "        Currently assumes that each land use has equivalent costs.\n",
    "\n",
    "\n",
    "        :param total_water:\n",
    "        :return: total water cost\n",
    "        \"\"\"\n",
    "        # TODO: ADJUST SO THAT THE COST FOR AQUIFER WITHDRAWALS IS DIFFERENT THAT SURFACE WATER\n",
    "        w_0 = 1e-5\n",
    "\n",
    "        cost = w_0 * total_water\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def land_cost_function(self, total_land):\n",
    "        \"\"\"\n",
    "        Non-Monotonic cost function for land.\n",
    "        Currently assumes that each land use has equivalent costs.\n",
    "\n",
    "\n",
    "        :param total_land: total land used by the farmer for both crops.\n",
    "        :return: total land cost\n",
    "        \"\"\"\n",
    "        w_0 = 1e-5\n",
    "\n",
    "        cost = w_0 * total_land\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def plot_reward(self, crop):\n",
    "\n",
    "        # TODO: MAKE THIS FLEXIBLE TO PLOT DIFFERENT CROPS\n",
    "\n",
    "        # crop 1\n",
    "        x_len = int(2e6)\n",
    "        # water crop 1\n",
    "        water = np.arange(x_len) + 1\n",
    "        # water crop 2\n",
    "        water = np.stack((water, np.zeros(x_len)), axis=-1)\n",
    "\n",
    "        # land crop 1, 100 total units available land for both crops\n",
    "        land = np.full(x_len, 100)\n",
    "\n",
    "        # land crop 2\n",
    "        land = np.stack((land, np.zeros(x_len)), axis=-1)\n",
    "\n",
    "        # combine water and land\n",
    "        action_vec = np.concatenate((water, land), axis=-1)\n",
    "\n",
    "        y = []\n",
    "        for action in action_vec:\n",
    "            y.append(self.reward_function(action))\n",
    "\n",
    "        x = action_vec[:, 0]\n",
    "\n",
    "        plt.title(\"Reward Function\")\n",
    "        plt.xlabel(\"Water\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.plot(x, y)\n",
    "        plt.show()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def init_available_water(self):\n",
    "        \"\"\"\n",
    "        Initializes available water for beginning of episode.\n",
    "        Pulls random sample from historic discharge distribution.\n",
    "\n",
    "        This needs to be a function call so that we get a new random number each time we want to initialize\n",
    "        a seasons available water.\n",
    "\n",
    "        Returns: (float) available water.\n",
    "        \"\"\"\n",
    "        # get initial values for water types\n",
    "        surface_water = np.random.choice(self.hydrology_function)\n",
    "        aquifer = self.aquifer.available_volume\n",
    "\n",
    "        # update water records\n",
    "        self.water_record[\"Surface\"].append(surface_water)\n",
    "        self.water_record[\"Aquifer\"].append(aquifer)\n",
    "\n",
    "        return surface_water + aquifer\n",
    "\n",
    "    def update_available_water(self, priority_index, action):\n",
    "        \"\"\"\n",
    "        Updates all available water down stream from agent indicated by priority index.\n",
    "\n",
    "        Args:\n",
    "            priority_index: indicates at what point in the river continuum to update flows\n",
    "            action: amount of water removed by agent\n",
    "        \"\"\"\n",
    "        # determine the total water removed\n",
    "        total_removed = np.sum(action[0:2])\n",
    "        self.farmers_water_withdrawal_record[priority_index].append(total_removed)\n",
    "\n",
    "        # remove water from aquifer\n",
    "        aquifer_removed = self.aquifer.withdraw_water(total_removed / 2)\n",
    "\n",
    "        # determine how much water needs to come from surface water\n",
    "        surface_removed = total_removed - aquifer_removed\n",
    "\n",
    "        # remove water from surface continuum, max operation prevents negative water amounts\n",
    "        for i in range(priority_index, len(self.available_water)):\n",
    "\n",
    "            self.available_water[i] = max(self.available_water[i] - surface_removed, 1e-3)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Takes a step in the environment and updates all relevant instance attributes.\n",
    "\n",
    "        This will look a little different than most other RL training loops for two reasons:\n",
    "        1. This is inherently a multi agent system\n",
    "        2. There is only one step in each episode\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "\n",
    "        # iterate through farmers in their priority order\n",
    "        for priority_num in self.farmer_priority:\n",
    "            agent = self.farmer_list[priority_num]\n",
    "            state = np.concatenate((\n",
    "                [self.available_water[priority_num]],\n",
    "                [self.available_land[priority_num]],\n",
    "                self.crop_prices)\n",
    "            )\n",
    "\n",
    "            # get action, and record actions for debugging\n",
    "            action = agent.act(state)\n",
    "            self.farmers_actions_record[priority_num].append(action)\n",
    "\n",
    "            reward = self.reward_function(action)\n",
    "            self.update_available_water(priority_num, action)\n",
    "\n",
    "            # save reward for later analysis\n",
    "            self.farmers_rewards_record[priority_num].append(reward)\n",
    "\n",
    "            # estimate next action and save state information in agent memory\n",
    "            next_state = np.concatenate((\n",
    "                [self.available_water[priority_num]],\n",
    "                [self.available_land[priority_num]],\n",
    "                self.crop_prices)\n",
    "            )\n",
    "            # TODO: ADJUST AGENT ALG TO REMOVE done STATE FIELD\n",
    "            agent.step(state, action, reward, next_state, True)\n",
    "\n",
    "        # 1 year of simulation complete, reset the source water\n",
    "        self.reset()\n",
    "        self.year += 1\n",
    "\n",
    "# for debugging\n",
    "config = SimConfig()\n",
    "env = SimulationCES(config)\n",
    "\n",
    "num_years = 1000\n",
    "\n",
    "for i_episode in range(1, num_years + 1):\n",
    "    env.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = SimConfig()\n",
    "test_env = SimulationCES(config)\n",
    "\n",
    "test_env.plot_reward(crop=\"crop 1\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = SimConfig()\n",
    "env = SimulationCES(config)\n",
    "\n",
    "num_years = 10000\n",
    "\n",
    "for i_episode in range(1, num_years + 1):\n",
    "    env.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# red and green should be screwed\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[0], 'b-', label=\"Farmer 1\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[1], 'r-', label=\"Farmer 2\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[2], 'g-', label=\"Farmer 3\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Water Withdrawn\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_water_withdrawal_record[0], 'b-', label=\"Farmer 1\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_water_withdrawal_record[1], 'r-', label=\"Farmer 2\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_water_withdrawal_record[2], 'g-', label=\"Farmer 3\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # test to make sure reward function was getting correct informaiton\n",
    "# farmer_1, farmer_2, farmer_3 = env.farmers_actions_record\n",
    "# farmer_1_y = []\n",
    "# for action in farmer_1:\n",
    "#     farmer_1_y.append(env.reward_function(action))\n",
    "#\n",
    "# farmer_3_y = []\n",
    "# for action in farmer_3:\n",
    "#     farmer_3_y.append(env.reward_function(action))\n",
    "#\n",
    "# x = np.arange(15)\n",
    "# plt.plot(x, farmer_1_y, label=\"Farmer 1\")\n",
    "# plt.plot(x, farmer_3_y, label=\"Farmer 3\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.title(\"Aquifer\")\n",
    "plt.plot(np.arange(env.year), env.water_record[\"Aquifer\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.title(\"Surface\")\n",
    "plt.plot(np.arange(env.year), env.water_record[\"Surface\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The observed noise in the reward is due to the stochastic nature of the available water distribution.\n",
    "In the config class you can switch between a constant water distribution, and a stochastic distribution to observe this.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# green should be screwed\n",
    "\n",
    "config = SimConfig()\n",
    "config.farmer_priority = [1,0,2]\n",
    "env = SimulationCES(config)\n",
    "\n",
    "num_years = 15\n",
    "\n",
    "for i_episode in range(1, num_years + 1):\n",
    "    env.step()\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[0], 'bo', label=\"Farmer 1\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[1], 'r-', label=\"Farmer 2\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[2], 'g-', label=\"Farmer 3\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# red should be screwed\n",
    "\n",
    "config = SimConfig()\n",
    "config.farmer_priority = [2,0,1]\n",
    "env = SimulationCES(config)\n",
    "\n",
    "num_years = 15\n",
    "\n",
    "for i_episode in range(1, num_years + 1):\n",
    "    env.step()\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[0], 'b-', label=\"Farmer 1\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[1], 'r-', label=\"Farmer 2\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[2], 'g-', label=\"Farmer 3\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# no body should be screwed\n",
    "\n",
    "config = SimConfig()\n",
    "config.farmer_priority = [2,1,0]\n",
    "env = SimulationCES(config)\n",
    "\n",
    "num_years = 15\n",
    "\n",
    "for i_episode in range(1, num_years + 1):\n",
    "    env.step()\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[0], 'b-', label=\"Farmer 1\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[1], 'r-', label=\"Farmer 2\")\n",
    "plt.plot(np.arange(env.year - 1), env.farmers_rewards_record[2], 'g-', label=\"Farmer 3\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The rewards should be reversed from what they are. There must be another bug somewhere in the simulation.\n",
    "\n",
    "TODO:\n",
    "\n",
    "1. simplify down to one farmer and one crop to make sure that the alg can find the max\n",
    "1. add simulation members to debug the reversed trend observed in the behavior from plots\n",
    "2. add in a regulatory agent\n",
    "3. adjust aquifer recharge rate\n",
    "4. incorporate the data from Marco's student, at least means first\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-dcb8c807",
   "language": "python",
   "display_name": "PyCharm (algs_spring_2020)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}